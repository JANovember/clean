{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import difflib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:26 - Data Cleaner - INFO - Logging configured successfully\n",
      "INFO:Data Cleaner:Logging configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "def setup_logger():\n",
    "    logger = logging.getLogger(\"Data Cleaner\")  # Create a logger object\n",
    "    if not logger.hasHandlers():  # Avoid adding handlers multiple times\n",
    "        logger.setLevel(logging.DEBUG)  # Set the minimum logging level\n",
    "\n",
    "        # Create handlers\n",
    "        file_handler = logging.FileHandler(\"breadcrumbs.log\")  # Logs to a file\n",
    "        console_handler = logging.StreamHandler()  # Logs to the VSCode terminal\n",
    "\n",
    "        # Set logging level for handlers\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Create formatters\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Initialize the logger\n",
    "logger = setup_logger()\n",
    "\n",
    "# Log a message\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path, sep=';', header=0, engine=None):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, header=header, engine=engine)\n",
    "        logger.info('File read, dataframe created')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error('File not read, dataframe not created')\n",
    "        print(f'Error reading csv: {e}')\n",
    "        return None\n",
    "    \n",
    "def remove_duplicates(df):\n",
    "    try:\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # Identify duplicates\n",
    "        duplicates = df[df.duplicated(keep=False)]\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        final_count = len(df)\n",
    "        \n",
    "        # Log and print the results\n",
    "        logger.info(f'Duplicates removed, {initial_count - final_count} duplicates removed')\n",
    "        print(f'Removed {initial_count - final_count} duplicate rows.')\n",
    "        \n",
    "        if not duplicates.empty:\n",
    "            print(\"\\nDropped Duplicates:\")\n",
    "            print(duplicates)\n",
    "            logger.info(f'Dropped duplicates:\\n{duplicates}')\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error removing duplicates, {e}')\n",
    "        print(f'Error removing duplicates: {e}')\n",
    "        return df\n",
    "\n",
    "\n",
    "def standardize_column_headers(df):\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '').str.replace('_', '')\n",
    "    logger.info(f'Columns headers standardized (spaces and underscores removed)')\n",
    "    return df\n",
    "\n",
    "def filter_dataframe(df, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to retain only specified columns, with error handling and logging.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns_to_keep (list): List of column names to retain.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A filtered DataFrame with only the specified columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate columns to keep\n",
    "        valid_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "        \n",
    "        if not valid_columns:\n",
    "            logging.warning(\"None of the specified columns are in the DataFrame. Returning an empty DataFrame.\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if no valid columns\n",
    "\n",
    "        # Log any missing columns\n",
    "        missing_columns = set(columns_to_keep) - set(valid_columns)\n",
    "        if missing_columns:\n",
    "            logging.warning(f\"The following columns were not found in the DataFrame: {missing_columns}\")\n",
    "\n",
    "        # Keep only the valid columns\n",
    "        filtered_df = df[valid_columns]\n",
    "        logging.info(f\"Successfully filtered DataFrame. Retained columns: {valid_columns}\")\n",
    "        return filtered_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while filtering the DataFrame: {e}\")\n",
    "        raise  # Re-raise the exception after logging\n",
    "\n",
    "def clean_and_standardize_names(df, column_name):\n",
    "    try:\n",
    "        logger.info(f'Name cleaning and standardization started for column: {column_name}')\n",
    "        def retain_language_characters(text):\n",
    "            \"\"\"\n",
    "            Retains all language-specific characters and removes everything else.\n",
    "            This includes Unicode letters from all scripts and spaces.\n",
    "            \"\"\"\n",
    "            if isinstance(text, str):\n",
    "                # Use regex to match Unicode letters (all scripts) and spaces\n",
    "                language_pattern = re.compile(r'[^\\p{L}\\s]', flags=re.UNICODE)\n",
    "                return language_pattern.sub('', text).strip()\n",
    "            return text\n",
    "\n",
    "        df[column_name] = df[column_name].apply(retain_language_characters)\n",
    "        df[column_name] = df[column_name].str.replace(r'\\d+', '', regex=True).str.strip()\n",
    "\n",
    "        def is_invalid_name(name: str) -> bool:\n",
    "            if pd.isna(name) or not isinstance(name, str): return True\n",
    "            if any(char.isdigit() for char in name) or any(char in set('~!@#$%^&*()_+=[]}{|\\\\:;,.<>?') for char in name):\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        invalid_names = df[df[column_name].apply(is_invalid_name)]\n",
    "        \n",
    "        def clean_name(name):\n",
    "            return '' if is_invalid_name(name) else name.title().strip()\n",
    "\n",
    "        df[column_name] = df[column_name].apply(clean_name)\n",
    "        logger.info(f'Name cleaning and standardization completed for column: {column_name}')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error standardizing names: {e}')\n",
    "        print(f'Error standardizing names: {e}')\n",
    "        return df\n",
    "\n",
    "def validate_emails(df, column):\n",
    "    try:\n",
    "        logger.info(f'Email validation started for column: {column}')\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@([^\\d@]+\\.[a-zA-Z]{2,})$'\n",
    "        valid_domains = ['gmail.com', 'hotmail.com', 'yahoo.com', 'outlook.com', 'aol.com', 'icloud.com','naver.com','naver.net','hanmail.net']\n",
    "        \n",
    "        df[column] = df[column].str.replace(' ', '', regex=False)\n",
    "\n",
    "        def clean_and_match_email(email):\n",
    "            if isinstance(email, str) and '@' in email:\n",
    "                username, domain = email.split('@', 1)\n",
    "                domain = re.sub(r'[^a-zA-Z0-9.-]', '', domain).strip().replace(' ', '').lower()\n",
    "                corrected_domain = difflib.get_close_matches(domain, valid_domains, n=1, cutoff=0.8)\n",
    "                if corrected_domain:\n",
    "                    domain = corrected_domain[0]\n",
    "                return username + '@' + domain\n",
    "            return email\n",
    "        \n",
    "        df[column] = df[column].apply(clean_and_match_email)\n",
    "        invalid_emails = df[~df[column].str.match(email_pattern, na=False)]\n",
    "\n",
    "        # Replace invalid emails with an empty string\n",
    "        df.loc[~df[column].str.match(email_pattern, na=False), column] = \"\"\n",
    "        print(f'Found {len(invalid_emails)} invalid email addresses.')\n",
    "        logger.info(f'Email validation completed for column: {column}')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error validating emails: {e}')\n",
    "        logger.error(f'Error validating emails: {e}')\n",
    "        return df\n",
    "    \n",
    "def validate_phone_numbers(df, column):\n",
    "    try:\n",
    "        logger.info(f'phone number validation started for column: {column}')\n",
    "        df[column] = df[column].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "        \n",
    "        def is_invalid_number(number: str) -> bool:\n",
    "            if not (6 <= len(number) <= 16) or bool(re.search(r'[^0-9+]', number)):\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        invalid_numbers = df[df[column].apply(is_invalid_number)]\n",
    "        df.loc[df[column].apply(is_invalid_number), column] = ''\n",
    "\n",
    "        print(f'Found {len(invalid_numbers)} invalid phone numbers.')\n",
    "        logger.info(f'phone number validation completed for column: {column}')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error validating phone numbers: {e}')\n",
    "        print(f'Error validating phone numbers: {e}')\n",
    "        return df\n",
    "        \n",
    "def can_not_be_empty(df):\n",
    "    # Specified columns to check for missing values\n",
    "    columns_to_check = [\"givenname\", \"surname\"]\n",
    "    \n",
    "    # Replace empty strings (\"\") with np.nan in the specified columns\n",
    "    df[columns_to_check] = df[columns_to_check].replace(\"\", np.nan)\n",
    "    \n",
    "    # Create a mask where rows with NaN in both 'givenname' and 'surname' are considered trash (True)\n",
    "    return df[columns_to_check].isna().all(axis=1)\n",
    "\n",
    "def has_three_consecutive_nulls_or_four_commas(row):\n",
    "    # Convert row to a string, then check for three consecutive commas\n",
    "    row_str = row.astype(str).str.cat(sep=',')\n",
    "    if ',,,,' in row_str:\n",
    "        return True\n",
    "\n",
    "    # Check for three consecutive NaNs in the row\n",
    "    nulls_as_list = row.isnull().astype(int).tolist()\n",
    "    return [1, 1, 1] in [nulls_as_list[i:i+3] for i in range(len(nulls_as_list) - 2)]\n",
    "\n",
    "def validate_rows(df):\n",
    "    try:\n",
    "        logger.info('Row validation started')\n",
    "\n",
    "        # Step 1: Create a mask for rows that should be dropped due to missing columns\n",
    "        empty_mask = can_not_be_empty(df)\n",
    "\n",
    "        # Step 2: Apply additional check for three consecutive nulls or four commas in any row\n",
    "        consecutive_nulls_or_commas_mask = df.apply(has_three_consecutive_nulls_or_four_commas, axis=1)\n",
    "\n",
    "        # Step 3: Combine the two masks to identify all rows to be dropped\n",
    "        rows_to_drop_mask = empty_mask | consecutive_nulls_or_commas_mask  # Rows that meet either condition\n",
    "\n",
    "        # Step 4: Filter the DataFrame into clean and trash data\n",
    "        clean_data = df[~rows_to_drop_mask]  # Keep rows that are not marked for dropping\n",
    "        trash_data = df[rows_to_drop_mask]   # Keep rows that are marked for dropping\n",
    "\n",
    "        # Step 5: Save the trash data to a file\n",
    "        trash_data.to_csv(\"7.garbage_rows.csv\", index=False)\n",
    "        logger.info('row validation completed')\n",
    "        return clean_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during row validation: {e}\")\n",
    "        raise  # Re-raise the exception to propagate it\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all required functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:26 - Data Cleaner - INFO - File read, dataframe created\n",
      "INFO:Data Cleaner:File read, dataframe created\n"
     ]
    }
   ],
   "source": [
    "df = read_csv(r'', sep=',', header=0, engine=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:26 - Data Cleaner - INFO - Columns headers standardized (spaces and underscores removed)\n",
      "INFO:Data Cleaner:Columns headers standardized (spaces and underscores removed)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = standardize_column_headers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = filter_dataframe(df, ['givenname', 'surname','mail', 'mobilephone', 'businessphones'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:26 - Data Cleaner - INFO - Duplicates removed, 8 duplicates removed\n",
      "INFO:Data Cleaner:Duplicates removed, 8 duplicates removed\n",
      "2025-01-20 12:21:26 - Data Cleaner - INFO - Dropped duplicates:\n",
      "      givenname                   surname  \\\n",
      "918       Cindy                  Amatuzzo   \n",
      "1047      Cindy                  Amatuzzo   \n",
      "3870        NaN                       NaN   \n",
      "4239        NaN                       NaN   \n",
      "4651      Brian                      Bock   \n",
      "5073      Brian                      Bock   \n",
      "7763     Vineet                     Kumar   \n",
      "9994     Vineet                     Kumar   \n",
      "12825     Group  Cash Support - NF String   \n",
      "12963     Group  Cash Support - NF String   \n",
      "14074       NaN                       NaN   \n",
      "14470       NaN                       NaN   \n",
      "19060       NaN                       NaN   \n",
      "20044       NaN                       NaN   \n",
      "20234       NaN                       NaN   \n",
      "20670       NaN                       NaN   \n",
      "\n",
      "                                              mail    mobilephone  \\\n",
      "918                      Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "1047                     Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "3870                         breakroom@McGriff.com            NaN   \n",
      "4239                         breakroom@McGriff.com            NaN   \n",
      "4651                         Brian.Bock@truist.com  (919)357-7044   \n",
      "5073                         Brian.Bock@truist.com  (919)357-7044   \n",
      "7763                                           NaN            NaN   \n",
      "9994                                           NaN            NaN   \n",
      "12825  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "12963  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "14074                CR-105241-02-02000@truist.com            NaN   \n",
      "14470                CR-105241-02-02000@truist.com            NaN   \n",
      "19060               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20044               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "20234               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20670               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "\n",
      "      businessphones  \n",
      "918    (704)499-5371  \n",
      "1047   (704)499-5371  \n",
      "3870    205-581-9305  \n",
      "4239    205-581-9305  \n",
      "4651    984-328-8996  \n",
      "5073    984-328-8996  \n",
      "7763    919-791-3262  \n",
      "9994    919-791-3262  \n",
      "12825            NaN  \n",
      "12963            NaN  \n",
      "14074            NaN  \n",
      "14470            NaN  \n",
      "19060            NaN  \n",
      "20044            NaN  \n",
      "20234            NaN  \n",
      "20670            NaN  \n",
      "INFO:Data Cleaner:Dropped duplicates:\n",
      "      givenname                   surname  \\\n",
      "918       Cindy                  Amatuzzo   \n",
      "1047      Cindy                  Amatuzzo   \n",
      "3870        NaN                       NaN   \n",
      "4239        NaN                       NaN   \n",
      "4651      Brian                      Bock   \n",
      "5073      Brian                      Bock   \n",
      "7763     Vineet                     Kumar   \n",
      "9994     Vineet                     Kumar   \n",
      "12825     Group  Cash Support - NF String   \n",
      "12963     Group  Cash Support - NF String   \n",
      "14074       NaN                       NaN   \n",
      "14470       NaN                       NaN   \n",
      "19060       NaN                       NaN   \n",
      "20044       NaN                       NaN   \n",
      "20234       NaN                       NaN   \n",
      "20670       NaN                       NaN   \n",
      "\n",
      "                                              mail    mobilephone  \\\n",
      "918                      Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "1047                     Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "3870                         breakroom@McGriff.com            NaN   \n",
      "4239                         breakroom@McGriff.com            NaN   \n",
      "4651                         Brian.Bock@truist.com  (919)357-7044   \n",
      "5073                         Brian.Bock@truist.com  (919)357-7044   \n",
      "7763                                           NaN            NaN   \n",
      "9994                                           NaN            NaN   \n",
      "12825  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "12963  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "14074                CR-105241-02-02000@truist.com            NaN   \n",
      "14470                CR-105241-02-02000@truist.com            NaN   \n",
      "19060               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20044               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "20234               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20670               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "\n",
      "      businessphones  \n",
      "918    (704)499-5371  \n",
      "1047   (704)499-5371  \n",
      "3870    205-581-9305  \n",
      "4239    205-581-9305  \n",
      "4651    984-328-8996  \n",
      "5073    984-328-8996  \n",
      "7763    919-791-3262  \n",
      "9994    919-791-3262  \n",
      "12825            NaN  \n",
      "12963            NaN  \n",
      "14074            NaN  \n",
      "14470            NaN  \n",
      "19060            NaN  \n",
      "20044            NaN  \n",
      "20234            NaN  \n",
      "20670            NaN  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 duplicate rows.\n",
      "\n",
      "Dropped Duplicates:\n",
      "      givenname                   surname  \\\n",
      "918       Cindy                  Amatuzzo   \n",
      "1047      Cindy                  Amatuzzo   \n",
      "3870        NaN                       NaN   \n",
      "4239        NaN                       NaN   \n",
      "4651      Brian                      Bock   \n",
      "5073      Brian                      Bock   \n",
      "7763     Vineet                     Kumar   \n",
      "9994     Vineet                     Kumar   \n",
      "12825     Group  Cash Support - NF String   \n",
      "12963     Group  Cash Support - NF String   \n",
      "14074       NaN                       NaN   \n",
      "14470       NaN                       NaN   \n",
      "19060       NaN                       NaN   \n",
      "20044       NaN                       NaN   \n",
      "20234       NaN                       NaN   \n",
      "20670       NaN                       NaN   \n",
      "\n",
      "                                              mail    mobilephone  \\\n",
      "918                      Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "1047                     Cindy.Amatuzzo@truist.com   716-479-7659   \n",
      "3870                         breakroom@McGriff.com            NaN   \n",
      "4239                         breakroom@McGriff.com            NaN   \n",
      "4651                         Brian.Bock@truist.com  (919)357-7044   \n",
      "5073                         Brian.Bock@truist.com  (919)357-7044   \n",
      "7763                                           NaN            NaN   \n",
      "9994                                           NaN            NaN   \n",
      "12825  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "12963  corporatereconcilement-cashrecon@truist.com            NaN   \n",
      "14074                CR-105241-02-02000@truist.com            NaN   \n",
      "14470                CR-105241-02-02000@truist.com            NaN   \n",
      "19060               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20044               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "20234               CCSGCoIAMGovernance@truist.com            NaN   \n",
      "20670               Calendar-CRIT-Stine@truist.com            NaN   \n",
      "\n",
      "      businessphones  \n",
      "918    (704)499-5371  \n",
      "1047   (704)499-5371  \n",
      "3870    205-581-9305  \n",
      "4239    205-581-9305  \n",
      "4651    984-328-8996  \n",
      "5073    984-328-8996  \n",
      "7763    919-791-3262  \n",
      "9994    919-791-3262  \n",
      "12825            NaN  \n",
      "12963            NaN  \n",
      "14074            NaN  \n",
      "14470            NaN  \n",
      "19060            NaN  \n",
      "20044            NaN  \n",
      "20234            NaN  \n",
      "20670            NaN  \n"
     ]
    }
   ],
   "source": [
    "df = remove_duplicates(df)\n",
    "df.to_csv('1.filtered&duplicatesremoved.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:26 - Data Cleaner - INFO - Name cleaning and standardization started for column: givenname\n",
      "INFO:Data Cleaner:Name cleaning and standardization started for column: givenname\n",
      "2025-01-20 12:21:28 - Data Cleaner - INFO - Name cleaning and standardization completed for column: givenname\n",
      "INFO:Data Cleaner:Name cleaning and standardization completed for column: givenname\n"
     ]
    }
   ],
   "source": [
    "df = clean_and_standardize_names(df, 'givenname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:28 - Data Cleaner - INFO - Name cleaning and standardization started for column: surname\n",
      "INFO:Data Cleaner:Name cleaning and standardization started for column: surname\n",
      "2025-01-20 12:21:32 - Data Cleaner - INFO - Name cleaning and standardization completed for column: surname\n",
      "INFO:Data Cleaner:Name cleaning and standardization completed for column: surname\n"
     ]
    }
   ],
   "source": [
    "df = clean_and_standardize_names(df, 'surname')\n",
    "df.to_csv('2.namesvalidated.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:32 - Data Cleaner - INFO - Email validation started for column: mail\n",
      "INFO:Data Cleaner:Email validation started for column: mail\n",
      "2025-01-20 12:21:33 - Data Cleaner - INFO - Email validation completed for column: mail\n",
      "INFO:Data Cleaner:Email validation completed for column: mail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 invalid email addresses.\n"
     ]
    }
   ],
   "source": [
    "df = validate_emails(df, 'mail')\n",
    "df.to_csv('3.emailsvalidated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:33 - Data Cleaner - INFO - phone number validation started for column: mobilephone\n",
      "INFO:Data Cleaner:phone number validation started for column: mobilephone\n",
      "2025-01-20 12:21:34 - Data Cleaner - INFO - phone number validation completed for column: mobilephone\n",
      "INFO:Data Cleaner:phone number validation completed for column: mobilephone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18803 invalid phone numbers.\n"
     ]
    }
   ],
   "source": [
    "df = validate_phone_numbers(df, 'mobilephone')   \n",
    "df.to_csv('4.phonesvalidated.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:34 - Data Cleaner - INFO - phone number validation started for column: businessphones\n",
      "INFO:Data Cleaner:phone number validation started for column: businessphones\n",
      "2025-01-20 12:21:34 - Data Cleaner - INFO - phone number validation completed for column: businessphones\n",
      "INFO:Data Cleaner:phone number validation completed for column: businessphones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11213 invalid phone numbers.\n"
     ]
    }
   ],
   "source": [
    "df = validate_phone_numbers(df, 'businessphones')   \n",
    "df.to_csv('5.businessphonesvalidated.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 12:21:35 - Data Cleaner - INFO - Row validation started\n",
      "INFO:Data Cleaner:Row validation started\n",
      "2025-01-20 12:21:42 - Data Cleaner - INFO - row validation completed\n",
      "INFO:Data Cleaner:row validation completed\n"
     ]
    }
   ],
   "source": [
    "df = validate_rows(df)\n",
    "df.to_csv('6.rowsvalidated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
